{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nimport math\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#El objetivo de este proyecto sera la prediccion de petal_width a partir las otras 3 caracteristicas medidas\n#en las flores de Iris,prescindiendo de la columna species\n#Para que nos puede llegar a servir este estudio?\n#Sabemos que para entrenar cualquier algoritmo cuantos mas datos tengamos mejor... pero tambien podemos imaginar que medir 1000 veces o mas \n#cada caracteristica de, en este caso cada flor,tiene algunas desventajas, a saber: errores en la medicion luego de estar varias horas midiendo\n#en donde este error puede ser instrumental o de la persona que mide, el tiempo invertido seria mucho,etc\n#Creo que en este caso seria util un algoritmo que a partir de 3 medidas nos permita predecir una cuarta medida.\n\niris_file='../input/iris-flower-dataset/IRIS.csv'\niris=pd.read_csv(iris_file)\niris.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#debido a que el objetivo es en este caso predecir petal_width, elimino la columna species\niris1=iris.drop([\"species\"], axis=1)\niris1.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris1.corr(method=\"pearson\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#quiero ver distribuciones individuales de cada una de mis variables,\n#a la vez que veo la relacion entre las variables (jointplot)\n\nfig=sns.jointplot(x=\"sepal_length\", y=\"sepal_width\", data=iris1, kind=\"reg\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=sns.jointplot(x=\"petal_length\", y=\"petal_width\", data=iris1, kind=\"reg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vamos a intentar predecir petal_width a partir de las otras 3 caracteristicas\n\ny=iris1.petal_width\ny.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=iris1.drop([\"petal_width\"], axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision de la metrica a utilizar: mse o mae?\n#Confecciono un Box plot\n\niris1.boxplot( figsize=(12,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Usaremos un arbol de decision para regresion y veremos los resultados\ntrain_X,test_X,train_y,test_y=train_test_split(X,y, test_size=0.2, random_state=1)\niris1_model=DecisionTreeRegressor(max_depth=5)\niris1_model.fit(train_X,train_y)\ntest_predictions=iris1_model.predict(test_X)\nprint(test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_absolute_error(test_predictions,test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_squared_error(test_predictions,test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae1=0.035464408010081085\nprint(math.sqrt(mae1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sepal_length=6.2\nsepal_width=3.4\npetal_length=5.2\n\nX_new1=pd.DataFrame({\"sepal_length\":[6.2],\"sepal_width\":[3.4],\"petal_length\":[5.2]})\niris1_model.predict(X_new1)\n#si variamos la variable sepal_width un poco, por ej de 3.4 a 3.7 vemos que la prediccion para petal_width\n#no varia, debido a la baja correlacion entre sepal_width y petal_width, en cambio si variamos solo un poco\n#sepal_length y petal_length vemos que enseguida petal_width cambia, debido a la fuerte correlacion entre estas\n#variables\nprint (X_new1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sepal_length=6.7\nsepal_width=3.1\npetal_length=5.6\n\nX_new1=pd.DataFrame({\"sepal_length\":[6.7],\"sepal_width\":[3.1],\"petal_length\":[5.6]})\niris1_model.predict(X_new1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"errores=pd.DataFrame({\"max_depth\":[0,5,10],\"mae\":[0.17999999,0.1415170,0.181666666],\n\"mse\":[0.0653333,0.03546,0.0660833333],\"rmse\":[0.2556038,0.18831996,0.25706678]})\nprint(errores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vemos que la profundidad maxima optima del arbol es de 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ahora probamos si las metricas mejoran con un random forest para regresion\n\ntrain_X,test_X,train_y,test_y=train_test_split(X,y, test_size=0.2, random_state=1)\niris1_model2=RandomForestRegressor(n_estimators=50)\niris1_model2.fit(train_X,train_y)\ntest_predictions2=iris1_model2.predict(test_X)\nprint(test_predictions2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_absolute_error(test_predictions2,test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_squared_error(test_predictions2,test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae1=0.033576112925925945\nprint(math.sqrt(mae1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generalmente el algoritmo Random Forest entrega mejores resultados que un Arbol de decision,como puede apreciarse.\n\nsepal_length=6.7\nsepal_width=3.1\npetal_length=5.6\n\nX_new2=pd.DataFrame({\"sepal_length\":[6.7],\"sepal_width\":[3.1],\"petal_length\":[5.6]})\niris1_model2.predict(X_new2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#el valor esperado es de 2.4, mientras que el valor predicho es de 2.28525\n#2.4-2.28525=0.11475","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"errores=pd.DataFrame({\"n_estimators\":[25,50,100],\"mae\":[0.126,0.125,0.120],\n\"mse\":[0.032,0.033,0.028],\"rmse\":[0.179,0.183,0.169]})\nprint(errores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#el n_estimador optimo ronda en los 100","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}